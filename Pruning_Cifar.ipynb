{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pruning_Cifar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMNhaX5MMIB73nrD7P+aEL4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahibsin/Pruning/blob/main/Pruning_Cifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPcGFvuo2hKQ",
        "outputId": "bb314827-8f82-4435-98b1-7e4412fc846b"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 17341241096145269910, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 17942366058709440438\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 4939117458982843402\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15695549568\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 13323596577904384198\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QakPhTQz2sdZ",
        "outputId": "149a78e4-3a46-4f83-fbe7-496a70e0e1ac"
      },
      "source": [
        "! rm -rf Pruning\n",
        "% ls \n",
        "! git clone https://github.com/sahibsin/Pruning.git\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "Cloning into 'Pruning'...\n",
            "remote: Enumerating objects: 259, done.\u001b[K\n",
            "remote: Counting objects: 100% (259/259), done.\u001b[K\n",
            "remote: Compressing objects: 100% (218/218), done.\u001b[K\n",
            "remote: Total 259 (delta 58), reused 237 (delta 36), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (259/259), 27.78 MiB | 10.05 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAx7UWs-23JH",
        "outputId": "72c52b18-0e65-4a75-aac2-1feb459122bb"
      },
      "source": [
        "!pwd\n",
        "% cd Pruning/\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/Pruning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f761n_r276V",
        "outputId": "73d3319d-c709-4742-ae3a-dbd5435f33cb"
      },
      "source": [
        "! python open_lth.py train --default_hparams=mnist_lenet_300_100"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================================================\n",
            "Training a Model (Replicate 1)\n",
            "----------------------------------------------------------------------------------\n",
            "Dataset Hyperparameters\n",
            "    * dataset_name => mnist\n",
            "    * batch_size => 128\n",
            "Model Hyperparameters\n",
            "    * model_name => mnist_lenet_300_100\n",
            "    * model_init => kaiming_normal\n",
            "    * batchnorm_init => uniform\n",
            "Training Hyperparameters\n",
            "    * optimizer_name => sgd\n",
            "    * lr => 0.1\n",
            "    * training_steps => 40ep\n",
            "Output Location: /root/open_lth_data2/train_574e51abc295d8da78175b320504f2ba/replicate_1/main\n",
            "==================================================================================\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/open_lth_datasets2/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "9920512it [00:03, 3143939.26it/s]                 \n",
            "Extracting /root/open_lth_datasets2/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /root/open_lth_datasets2/mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/open_lth_datasets2/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "32768it [00:00, 52264.99it/s]               \n",
            "Extracting /root/open_lth_datasets2/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /root/open_lth_datasets2/mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/open_lth_datasets2/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "1654784it [00:01, 895321.81it/s]                 \n",
            "Extracting /root/open_lth_datasets2/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/open_lth_datasets2/mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/open_lth_datasets2/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "8192it [00:00, 18310.98it/s]\n",
            "Extracting /root/open_lth_datasets2/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/open_lth_datasets2/mnist/MNIST/raw\n",
            "Processing...\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
            "Done!\n",
            "test\tep 000 it 000 loss 4.079 acc1 9.07% acc5 56.47% ex 10000 time 0.00s\n",
            "test\tep 001 it 000 loss 0.146 acc1 95.47% acc5 99.86% ex 10000 time 17.58s\n",
            "test\tep 002 it 000 loss 0.116 acc1 96.44% acc5 99.90% ex 10000 time 17.56s\n",
            "test\tep 003 it 000 loss 0.090 acc1 97.25% acc5 99.95% ex 10000 time 17.57s\n",
            "test\tep 004 it 000 loss 0.093 acc1 97.09% acc5 99.95% ex 10000 time 17.72s\n",
            "test\tep 005 it 000 loss 0.083 acc1 97.33% acc5 99.96% ex 10000 time 17.68s\n",
            "test\tep 006 it 000 loss 0.079 acc1 97.56% acc5 99.97% ex 10000 time 17.61s\n",
            "test\tep 007 it 000 loss 0.089 acc1 97.27% acc5 99.95% ex 10000 time 17.48s\n",
            "test\tep 008 it 000 loss 0.075 acc1 97.83% acc5 99.97% ex 10000 time 17.58s\n",
            "test\tep 009 it 000 loss 0.077 acc1 97.80% acc5 99.96% ex 10000 time 17.53s\n",
            "test\tep 010 it 000 loss 0.090 acc1 97.49% acc5 99.94% ex 10000 time 17.48s\n",
            "test\tep 011 it 000 loss 0.076 acc1 97.78% acc5 99.96% ex 10000 time 17.45s\n",
            "test\tep 012 it 000 loss 0.074 acc1 97.87% acc5 99.97% ex 10000 time 17.49s\n",
            "test\tep 013 it 000 loss 0.076 acc1 97.97% acc5 99.97% ex 10000 time 17.66s\n",
            "test\tep 014 it 000 loss 0.076 acc1 97.92% acc5 99.97% ex 10000 time 17.66s\n",
            "test\tep 015 it 000 loss 0.077 acc1 97.88% acc5 99.96% ex 10000 time 17.54s\n",
            "test\tep 016 it 000 loss 0.077 acc1 97.90% acc5 99.97% ex 10000 time 17.69s\n",
            "test\tep 017 it 000 loss 0.077 acc1 97.91% acc5 99.97% ex 10000 time 17.63s\n",
            "test\tep 018 it 000 loss 0.078 acc1 97.85% acc5 99.96% ex 10000 time 17.45s\n",
            "test\tep 019 it 000 loss 0.079 acc1 97.95% acc5 99.96% ex 10000 time 17.53s\n",
            "test\tep 020 it 000 loss 0.079 acc1 97.96% acc5 99.96% ex 10000 time 17.66s\n",
            "test\tep 021 it 000 loss 0.079 acc1 97.98% acc5 99.96% ex 10000 time 17.76s\n",
            "test\tep 022 it 000 loss 0.079 acc1 97.99% acc5 99.96% ex 10000 time 17.51s\n",
            "test\tep 023 it 000 loss 0.081 acc1 98.01% acc5 99.96% ex 10000 time 17.54s\n",
            "test\tep 024 it 000 loss 0.081 acc1 97.95% acc5 99.96% ex 10000 time 17.47s\n",
            "test\tep 025 it 000 loss 0.081 acc1 97.93% acc5 99.96% ex 10000 time 17.72s\n",
            "test\tep 026 it 000 loss 0.082 acc1 97.97% acc5 99.96% ex 10000 time 17.55s\n",
            "test\tep 027 it 000 loss 0.083 acc1 97.98% acc5 99.96% ex 10000 time 17.62s\n",
            "test\tep 028 it 000 loss 0.083 acc1 97.96% acc5 99.96% ex 10000 time 17.51s\n",
            "test\tep 029 it 000 loss 0.083 acc1 97.92% acc5 99.96% ex 10000 time 17.24s\n",
            "test\tep 030 it 000 loss 0.084 acc1 97.99% acc5 99.96% ex 10000 time 17.66s\n",
            "test\tep 031 it 000 loss 0.085 acc1 97.97% acc5 99.96% ex 10000 time 17.56s\n",
            "test\tep 032 it 000 loss 0.084 acc1 97.95% acc5 99.96% ex 10000 time 17.58s\n",
            "test\tep 033 it 000 loss 0.084 acc1 98.00% acc5 99.96% ex 10000 time 17.47s\n",
            "test\tep 034 it 000 loss 0.085 acc1 97.97% acc5 99.96% ex 10000 time 17.66s\n",
            "test\tep 035 it 000 loss 0.085 acc1 97.98% acc5 99.96% ex 10000 time 17.52s\n",
            "test\tep 036 it 000 loss 0.086 acc1 97.99% acc5 99.96% ex 10000 time 17.42s\n",
            "test\tep 037 it 000 loss 0.086 acc1 97.96% acc5 99.96% ex 10000 time 17.56s\n",
            "test\tep 038 it 000 loss 0.086 acc1 97.99% acc5 99.96% ex 10000 time 17.60s\n",
            "test\tep 039 it 000 loss 0.087 acc1 97.96% acc5 99.96% ex 10000 time 17.72s\n",
            "test\tep 040 it 000 loss 0.087 acc1 97.99% acc5 99.96% ex 10000 time 17.47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cBxbPi52_kt",
        "outputId": "1a664c1f-6bc6-449b-8155-d84a7c7b7336"
      },
      "source": [
        "!python open_lth.py branch train oneshot --default_hparams=mnist_lenet_300_100 --strategy=magnitude --prune_fraction=0.75\n",
        "!python open_lth.py branch train oneshot --default_hparams=mnist_lenet_300_100 --strategy=random --prune_fraction=0.75\n",
        "!python open_lth.py branch train oneshot --default_hparams=mnist_lenet_300_100 --strategy=snip10 --prune_fraction=0.75\n",
        "!python open_lth.py branch train oneshot --default_hparams=mnist_lenet_300_100 --strategy=grasp10 --prune_fraction=0.75\n",
        "!python open_lth.py branch train oneshot --default_hparams=mnist_lenet_300_100 --strategy=graspabs10 --prune_fraction=0.75\n",
        "!python open_lth.py branch train oneshot --default_hparams=mnist_lenet_300_100 --strategy=synflow --prune_fraction=0.75\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================================================\n",
            "Branch oneshot_experiments (Replicate 1)\n",
            "----------------------------------------------------------------------------------\n",
            "Dataset Hyperparameters\n",
            "    * dataset_name => mnist\n",
            "    * batch_size => 128\n",
            "Model Hyperparameters\n",
            "    * model_name => mnist_lenet_300_100\n",
            "    * model_init => kaiming_normal\n",
            "    * batchnorm_init => uniform\n",
            "Training Hyperparameters\n",
            "    * optimizer_name => sgd\n",
            "    * lr => 0.1\n",
            "    * training_steps => 40ep\n",
            "Branch Arguments\n",
            "    * strategy => magnitude\n",
            "    * prune_fraction => 0.75\n",
            "    * prune_experiment => main\n",
            "    * prune_step => 0ep0it\n",
            "    * prune_iterations => 1\n",
            "    * state_experiment => main\n",
            "    * state_step => 0ep0it\n",
            "    * start_step => 0ep0it\n",
            "Output Location: /root/open_lth_data2/train_574e51abc295d8da78175b320504f2ba/replicate_1/train_branch_oneshot_experiments_8df967e2afc55058b744b4c1f8dab80d\n",
            "==================================================================================\n",
            "\n",
            "prune iteration 1\n",
            "\n",
            "Magnitude Pruning | fc_layers.0.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.0.weight, Layer Size: 235200\n",
            "        Min Score: 7.296927151401178e-08\n",
            "        Average Score: 0.040272265672683716\n",
            "        Median Score: 0.03401786461472511\n",
            "        Max Score: 0.2723902761936188  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\n",
            "Magnitude Pruning | fc_layers.1.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.1.weight, Layer Size: 30000\n",
            "        Min Score: 1.2206609426357318e-05\n",
            "        Average Score: 0.06494694203138351\n",
            "        Median Score: 0.05471707880496979\n",
            "        Max Score: 0.30655598640441895  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\n",
            "Magnitude Pruning | fc.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc.weight, Layer Size: 1000\n",
            "        Min Score: 0.00021808716701343656\n",
            "        Average Score: 0.11446354538202286\n",
            "        Median Score: 0.10289596021175385\n",
            "        Max Score: 0.4283660650253296  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Training\n",
            "test\tep 000 it 000 loss 3.797 acc1 8.95% acc5 54.63% ex 10000 time 0.00s\n",
            "test\tep 001 it 000 loss 0.210 acc1 93.75% acc5 99.70% ex 10000 time 17.85s\n",
            "test\tep 002 it 000 loss 0.157 acc1 95.21% acc5 99.82% ex 10000 time 17.92s\n",
            "test\tep 003 it 000 loss 0.138 acc1 95.72% acc5 99.88% ex 10000 time 17.81s\n",
            "test\tep 004 it 000 loss 0.117 acc1 96.48% acc5 99.92% ex 10000 time 17.68s\n",
            "test\tep 005 it 000 loss 0.103 acc1 96.84% acc5 99.94% ex 10000 time 17.54s\n",
            "test\tep 006 it 000 loss 0.104 acc1 96.90% acc5 99.93% ex 10000 time 17.69s\n",
            "test\tep 007 it 000 loss 0.100 acc1 96.99% acc5 99.94% ex 10000 time 17.99s\n",
            "test\tep 008 it 000 loss 0.100 acc1 96.98% acc5 99.93% ex 10000 time 17.85s\n",
            "test\tep 009 it 000 loss 0.091 acc1 97.07% acc5 99.94% ex 10000 time 17.83s\n",
            "test\tep 010 it 000 loss 0.090 acc1 97.23% acc5 99.95% ex 10000 time 17.88s\n",
            "test\tep 011 it 000 loss 0.083 acc1 97.34% acc5 99.94% ex 10000 time 17.91s\n",
            "test\tep 012 it 000 loss 0.088 acc1 97.22% acc5 99.95% ex 10000 time 17.99s\n",
            "test\tep 013 it 000 loss 0.084 acc1 97.47% acc5 99.95% ex 10000 time 18.35s\n",
            "test\tep 014 it 000 loss 0.085 acc1 97.40% acc5 99.94% ex 10000 time 18.24s\n",
            "test\tep 015 it 000 loss 0.084 acc1 97.42% acc5 99.96% ex 10000 time 17.98s\n",
            "test\tep 016 it 000 loss 0.084 acc1 97.42% acc5 99.95% ex 10000 time 17.96s\n",
            "test\tep 017 it 000 loss 0.083 acc1 97.49% acc5 99.94% ex 10000 time 17.83s\n",
            "test\tep 018 it 000 loss 0.087 acc1 97.38% acc5 99.94% ex 10000 time 17.91s\n",
            "test\tep 019 it 000 loss 0.085 acc1 97.40% acc5 99.94% ex 10000 time 17.81s\n",
            "test\tep 020 it 000 loss 0.087 acc1 97.42% acc5 99.94% ex 10000 time 17.86s\n",
            "test\tep 021 it 000 loss 0.087 acc1 97.39% acc5 99.94% ex 10000 time 17.79s\n",
            "test\tep 022 it 000 loss 0.088 acc1 97.38% acc5 99.95% ex 10000 time 17.77s\n",
            "test\tep 023 it 000 loss 0.086 acc1 97.44% acc5 99.94% ex 10000 time 17.86s\n",
            "test\tep 024 it 000 loss 0.087 acc1 97.48% acc5 99.94% ex 10000 time 17.91s\n",
            "test\tep 025 it 000 loss 0.088 acc1 97.44% acc5 99.94% ex 10000 time 17.89s\n",
            "test\tep 026 it 000 loss 0.089 acc1 97.49% acc5 99.93% ex 10000 time 17.84s\n",
            "test\tep 027 it 000 loss 0.090 acc1 97.49% acc5 99.94% ex 10000 time 17.87s\n",
            "test\tep 028 it 000 loss 0.089 acc1 97.43% acc5 99.94% ex 10000 time 17.76s\n",
            "test\tep 029 it 000 loss 0.090 acc1 97.47% acc5 99.94% ex 10000 time 17.90s\n",
            "test\tep 030 it 000 loss 0.092 acc1 97.50% acc5 99.93% ex 10000 time 17.86s\n",
            "test\tep 031 it 000 loss 0.092 acc1 97.48% acc5 99.94% ex 10000 time 17.99s\n",
            "test\tep 032 it 000 loss 0.093 acc1 97.46% acc5 99.94% ex 10000 time 17.95s\n",
            "test\tep 033 it 000 loss 0.092 acc1 97.50% acc5 99.94% ex 10000 time 17.89s\n",
            "test\tep 034 it 000 loss 0.093 acc1 97.51% acc5 99.94% ex 10000 time 17.84s\n",
            "test\tep 035 it 000 loss 0.094 acc1 97.48% acc5 99.94% ex 10000 time 17.75s\n",
            "test\tep 036 it 000 loss 0.094 acc1 97.50% acc5 99.94% ex 10000 time 17.90s\n",
            "test\tep 037 it 000 loss 0.095 acc1 97.48% acc5 99.94% ex 10000 time 17.85s\n",
            "test\tep 038 it 000 loss 0.096 acc1 97.47% acc5 99.94% ex 10000 time 17.82s\n",
            "test\tep 039 it 000 loss 0.096 acc1 97.48% acc5 99.94% ex 10000 time 17.65s\n",
            "test\tep 040 it 000 loss 0.096 acc1 97.54% acc5 99.94% ex 10000 time 17.73s\n",
            "==================================================================================\n",
            "Branch oneshot_experiments (Replicate 1)\n",
            "----------------------------------------------------------------------------------\n",
            "Dataset Hyperparameters\n",
            "    * dataset_name => mnist\n",
            "    * batch_size => 128\n",
            "Model Hyperparameters\n",
            "    * model_name => mnist_lenet_300_100\n",
            "    * model_init => kaiming_normal\n",
            "    * batchnorm_init => uniform\n",
            "Training Hyperparameters\n",
            "    * optimizer_name => sgd\n",
            "    * lr => 0.1\n",
            "    * training_steps => 40ep\n",
            "Branch Arguments\n",
            "    * strategy => random\n",
            "    * prune_fraction => 0.75\n",
            "    * prune_experiment => main\n",
            "    * prune_step => 0ep0it\n",
            "    * prune_iterations => 1\n",
            "    * state_experiment => main\n",
            "    * state_step => 0ep0it\n",
            "    * start_step => 0ep0it\n",
            "Output Location: /root/open_lth_data2/train_574e51abc295d8da78175b320504f2ba/replicate_1/train_branch_oneshot_experiments_e2561760d8dbdfb935471c2ac331de7f\n",
            "==================================================================================\n",
            "\n",
            "prune iteration 1\n",
            "\n",
            "Random Pruning | fc.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc.weight, Layer Size: 1000\n",
            "        Min Score: 0.003657996654510498\n",
            "        Average Score: 0.5024212002754211\n",
            "        Median Score: 0.49729686975479126\n",
            "        Max Score: 0.9996365308761597  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\n",
            "Random Pruning | fc_layers.0.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.0.weight, Layer Size: 235200\n",
            "        Min Score: 1.2576580047607422e-05\n",
            "        Average Score: 0.4999734163284302\n",
            "        Median Score: 0.5005659461021423\n",
            "        Max Score: 0.9999973773956299  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\n",
            "Random Pruning | fc_layers.1.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.1.weight, Layer Size: 30000\n",
            "        Min Score: 2.086162567138672e-06\n",
            "        Average Score: 0.5004340410232544\n",
            "        Median Score: 0.502106785774231\n",
            "        Max Score: 0.9999595880508423  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Training\n",
            "test\tep 000 it 000 loss 2.306 acc1 6.91% acc5 53.18% ex 10000 time 0.00s\n",
            "test\tep 001 it 000 loss 0.252 acc1 92.69% acc5 99.58% ex 10000 time 18.03s\n",
            "test\tep 002 it 000 loss 0.183 acc1 94.68% acc5 99.78% ex 10000 time 18.01s\n",
            "test\tep 003 it 000 loss 0.154 acc1 95.40% acc5 99.83% ex 10000 time 17.95s\n",
            "test\tep 004 it 000 loss 0.127 acc1 96.22% acc5 99.88% ex 10000 time 18.01s\n",
            "test\tep 005 it 000 loss 0.108 acc1 96.76% acc5 99.90% ex 10000 time 18.10s\n",
            "test\tep 006 it 000 loss 0.103 acc1 96.81% acc5 99.91% ex 10000 time 18.09s\n",
            "test\tep 007 it 000 loss 0.096 acc1 97.04% acc5 99.91% ex 10000 time 18.06s\n",
            "test\tep 008 it 000 loss 0.092 acc1 97.07% acc5 99.94% ex 10000 time 18.07s\n",
            "test\tep 009 it 000 loss 0.085 acc1 97.24% acc5 99.94% ex 10000 time 18.12s\n",
            "test\tep 010 it 000 loss 0.083 acc1 97.41% acc5 99.94% ex 10000 time 18.25s\n",
            "test\tep 011 it 000 loss 0.078 acc1 97.46% acc5 99.96% ex 10000 time 18.07s\n",
            "test\tep 012 it 000 loss 0.085 acc1 97.27% acc5 99.97% ex 10000 time 18.07s\n",
            "test\tep 013 it 000 loss 0.081 acc1 97.40% acc5 99.94% ex 10000 time 17.99s\n",
            "test\tep 014 it 000 loss 0.077 acc1 97.46% acc5 99.96% ex 10000 time 18.06s\n",
            "test\tep 015 it 000 loss 0.080 acc1 97.46% acc5 99.94% ex 10000 time 17.88s\n",
            "test\tep 016 it 000 loss 0.079 acc1 97.53% acc5 99.95% ex 10000 time 17.98s\n",
            "test\tep 017 it 000 loss 0.075 acc1 97.59% acc5 99.97% ex 10000 time 18.00s\n",
            "test\tep 018 it 000 loss 0.077 acc1 97.57% acc5 99.96% ex 10000 time 17.97s\n",
            "test\tep 019 it 000 loss 0.076 acc1 97.52% acc5 99.96% ex 10000 time 17.93s\n",
            "test\tep 020 it 000 loss 0.077 acc1 97.62% acc5 99.96% ex 10000 time 18.04s\n",
            "test\tep 021 it 000 loss 0.076 acc1 97.60% acc5 99.95% ex 10000 time 17.95s\n",
            "test\tep 022 it 000 loss 0.075 acc1 97.70% acc5 99.96% ex 10000 time 17.90s\n",
            "test\tep 023 it 000 loss 0.080 acc1 97.54% acc5 99.95% ex 10000 time 18.02s\n",
            "test\tep 024 it 000 loss 0.080 acc1 97.55% acc5 99.96% ex 10000 time 17.96s\n",
            "test\tep 025 it 000 loss 0.078 acc1 97.64% acc5 99.97% ex 10000 time 18.08s\n",
            "test\tep 026 it 000 loss 0.080 acc1 97.62% acc5 99.96% ex 10000 time 18.04s\n",
            "test\tep 027 it 000 loss 0.078 acc1 97.70% acc5 99.95% ex 10000 time 18.12s\n",
            "test\tep 028 it 000 loss 0.080 acc1 97.66% acc5 99.94% ex 10000 time 18.02s\n",
            "test\tep 029 it 000 loss 0.081 acc1 97.65% acc5 99.95% ex 10000 time 17.92s\n",
            "test\tep 030 it 000 loss 0.080 acc1 97.64% acc5 99.95% ex 10000 time 18.06s\n",
            "test\tep 031 it 000 loss 0.080 acc1 97.65% acc5 99.97% ex 10000 time 17.83s\n",
            "test\tep 032 it 000 loss 0.083 acc1 97.63% acc5 99.96% ex 10000 time 17.88s\n",
            "test\tep 033 it 000 loss 0.082 acc1 97.66% acc5 99.95% ex 10000 time 17.97s\n",
            "test\tep 034 it 000 loss 0.082 acc1 97.68% acc5 99.95% ex 10000 time 18.07s\n",
            "test\tep 035 it 000 loss 0.083 acc1 97.77% acc5 99.95% ex 10000 time 18.01s\n",
            "test\tep 036 it 000 loss 0.084 acc1 97.57% acc5 99.96% ex 10000 time 18.02s\n",
            "test\tep 037 it 000 loss 0.085 acc1 97.71% acc5 99.95% ex 10000 time 17.93s\n",
            "test\tep 038 it 000 loss 0.085 acc1 97.77% acc5 99.96% ex 10000 time 17.88s\n",
            "test\tep 039 it 000 loss 0.085 acc1 97.73% acc5 99.96% ex 10000 time 17.93s\n",
            "test\tep 040 it 000 loss 0.086 acc1 97.69% acc5 99.95% ex 10000 time 17.84s\n",
            "==================================================================================\n",
            "Branch oneshot_experiments (Replicate 1)\n",
            "----------------------------------------------------------------------------------\n",
            "Dataset Hyperparameters\n",
            "    * dataset_name => mnist\n",
            "    * batch_size => 128\n",
            "Model Hyperparameters\n",
            "    * model_name => mnist_lenet_300_100\n",
            "    * model_init => kaiming_normal\n",
            "    * batchnorm_init => uniform\n",
            "Training Hyperparameters\n",
            "    * optimizer_name => sgd\n",
            "    * lr => 0.1\n",
            "    * training_steps => 40ep\n",
            "Branch Arguments\n",
            "    * strategy => snip10\n",
            "    * prune_fraction => 0.75\n",
            "    * prune_experiment => main\n",
            "    * prune_step => 0ep0it\n",
            "    * prune_iterations => 1\n",
            "    * state_experiment => main\n",
            "    * state_step => 0ep0it\n",
            "    * start_step => 0ep0it\n",
            "Output Location: /root/open_lth_data2/train_574e51abc295d8da78175b320504f2ba/replicate_1/train_branch_oneshot_experiments_51daa361d077ecb4d562ec1621ba50d4\n",
            "==================================================================================\n",
            "\n",
            "prune iteration 1\n",
            "snip 0\n",
            "\n",
            "Snip10 Pruning | fc_layers.0.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.0.weight, Layer Size: 235200\n",
            "        Min Score: 6.79201070385993e-11\n",
            "        Average Score: 0.000525974144693464\n",
            "        Median Score: 0.00022179263760335743\n",
            "        Max Score: 0.022317420691251755  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\n",
            "Snip10 Pruning | fc_layers.1.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.1.weight, Layer Size: 30000\n",
            "        Min Score: 0.0\n",
            "        Average Score: 0.001977968029677868\n",
            "        Median Score: 0.0006115855649113655\n",
            "        Max Score: 0.07452460378408432  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\n",
            "Snip10 Pruning | fc.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc.weight, Layer Size: 1000\n",
            "        Min Score: 1.126516324667648e-09\n",
            "        Average Score: 0.010491170920431614\n",
            "        Median Score: 0.0029410291463136673\n",
            "        Max Score: 0.2919114828109741  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Training\n",
            "test\tep 000 it 000 loss 4.731 acc1 8.92% acc5 52.62% ex 10000 time 0.00s\n",
            "test\tep 001 it 000 loss 0.188 acc1 94.33% acc5 99.79% ex 10000 time 17.92s\n",
            "test\tep 002 it 000 loss 0.137 acc1 95.70% acc5 99.90% ex 10000 time 17.98s\n",
            "test\tep 003 it 000 loss 0.112 acc1 96.48% acc5 99.93% ex 10000 time 18.08s\n",
            "test\tep 004 it 000 loss 0.098 acc1 97.08% acc5 99.94% ex 10000 time 18.15s\n",
            "test\tep 005 it 000 loss 0.090 acc1 97.02% acc5 99.92% ex 10000 time 18.02s\n",
            "test\tep 006 it 000 loss 0.085 acc1 97.22% acc5 99.95% ex 10000 time 18.11s\n",
            "test\tep 007 it 000 loss 0.085 acc1 97.33% acc5 99.93% ex 10000 time 17.96s\n",
            "test\tep 008 it 000 loss 0.080 acc1 97.28% acc5 99.96% ex 10000 time 17.93s\n",
            "test\tep 009 it 000 loss 0.087 acc1 97.39% acc5 99.94% ex 10000 time 18.02s\n",
            "test\tep 010 it 000 loss 0.079 acc1 97.44% acc5 99.94% ex 10000 time 18.01s\n",
            "test\tep 011 it 000 loss 0.077 acc1 97.52% acc5 99.96% ex 10000 time 17.94s\n",
            "test\tep 012 it 000 loss 0.085 acc1 97.51% acc5 99.95% ex 10000 time 18.06s\n",
            "test\tep 013 it 000 loss 0.085 acc1 97.43% acc5 99.95% ex 10000 time 17.87s\n",
            "test\tep 014 it 000 loss 0.076 acc1 97.62% acc5 99.97% ex 10000 time 18.04s\n",
            "test\tep 015 it 000 loss 0.078 acc1 97.53% acc5 99.97% ex 10000 time 18.04s\n",
            "test\tep 016 it 000 loss 0.091 acc1 97.38% acc5 99.95% ex 10000 time 18.00s\n",
            "test\tep 017 it 000 loss 0.083 acc1 97.49% acc5 99.93% ex 10000 time 18.06s\n",
            "test\tep 018 it 000 loss 0.081 acc1 97.57% acc5 99.96% ex 10000 time 18.08s\n",
            "test\tep 019 it 000 loss 0.081 acc1 97.61% acc5 99.96% ex 10000 time 18.10s\n",
            "test\tep 020 it 000 loss 0.082 acc1 97.62% acc5 99.95% ex 10000 time 17.96s\n",
            "test\tep 021 it 000 loss 0.082 acc1 97.65% acc5 99.96% ex 10000 time 18.42s\n",
            "test\tep 022 it 000 loss 0.083 acc1 97.66% acc5 99.95% ex 10000 time 18.17s\n",
            "test\tep 023 it 000 loss 0.082 acc1 97.75% acc5 99.96% ex 10000 time 18.13s\n",
            "test\tep 024 it 000 loss 0.085 acc1 97.63% acc5 99.95% ex 10000 time 17.97s\n",
            "test\tep 025 it 000 loss 0.085 acc1 97.60% acc5 99.96% ex 10000 time 17.92s\n",
            "test\tep 026 it 000 loss 0.085 acc1 97.71% acc5 99.96% ex 10000 time 17.90s\n",
            "test\tep 027 it 000 loss 0.087 acc1 97.67% acc5 99.95% ex 10000 time 17.97s\n",
            "test\tep 028 it 000 loss 0.086 acc1 97.69% acc5 99.96% ex 10000 time 17.89s\n",
            "test\tep 029 it 000 loss 0.086 acc1 97.75% acc5 99.96% ex 10000 time 17.90s\n",
            "test\tep 030 it 000 loss 0.087 acc1 97.71% acc5 99.96% ex 10000 time 17.84s\n",
            "test\tep 031 it 000 loss 0.087 acc1 97.78% acc5 99.95% ex 10000 time 18.08s\n",
            "test\tep 032 it 000 loss 0.089 acc1 97.83% acc5 99.94% ex 10000 time 17.98s\n",
            "test\tep 033 it 000 loss 0.088 acc1 97.81% acc5 99.95% ex 10000 time 18.10s\n",
            "test\tep 034 it 000 loss 0.091 acc1 97.67% acc5 99.96% ex 10000 time 17.97s\n",
            "test\tep 035 it 000 loss 0.090 acc1 97.73% acc5 99.96% ex 10000 time 18.07s\n",
            "test\tep 036 it 000 loss 0.090 acc1 97.72% acc5 99.95% ex 10000 time 17.91s\n",
            "test\tep 037 it 000 loss 0.091 acc1 97.73% acc5 99.96% ex 10000 time 18.23s\n",
            "test\tep 038 it 000 loss 0.091 acc1 97.72% acc5 99.96% ex 10000 time 18.21s\n",
            "test\tep 039 it 000 loss 0.092 acc1 97.72% acc5 99.95% ex 10000 time 18.09s\n",
            "test\tep 040 it 000 loss 0.093 acc1 97.76% acc5 99.95% ex 10000 time 18.23s\n",
            "==================================================================================\n",
            "Branch oneshot_experiments (Replicate 1)\n",
            "----------------------------------------------------------------------------------\n",
            "Dataset Hyperparameters\n",
            "    * dataset_name => mnist\n",
            "    * batch_size => 128\n",
            "Model Hyperparameters\n",
            "    * model_name => mnist_lenet_300_100\n",
            "    * model_init => kaiming_normal\n",
            "    * batchnorm_init => uniform\n",
            "Training Hyperparameters\n",
            "    * optimizer_name => sgd\n",
            "    * lr => 0.1\n",
            "    * training_steps => 40ep\n",
            "Branch Arguments\n",
            "    * strategy => grasp10\n",
            "    * prune_fraction => 0.75\n",
            "    * prune_experiment => main\n",
            "    * prune_step => 0ep0it\n",
            "    * prune_iterations => 1\n",
            "    * state_experiment => main\n",
            "    * state_step => 0ep0it\n",
            "    * start_step => 0ep0it\n",
            "Output Location: /root/open_lth_data2/train_574e51abc295d8da78175b320504f2ba/replicate_1/train_branch_oneshot_experiments_dad30d8dc7c7708ce3caa68b7860564c\n",
            "==================================================================================\n",
            "\n",
            "prune iteration 1\n",
            "grasp 0 \n",
            "\n",
            "Grasp10 Pruning | fc_layers.0.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.0.weight, Layer Size: 235200\n",
            "        Min Score: -2.5682084014988504e-07\n",
            "        Average Score: -6.460677548503213e-10\n",
            "        Median Score: -3.9572303767165806e-11\n",
            "        Max Score: 2.6075935011249385e-07  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "\n",
            "Grasp10 Pruning | fc_layers.1.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.1.weight, Layer Size: 30000\n",
            "        Min Score: -1.3496537576429546e-06\n",
            "        Average Score: -1.2702426666066913e-08\n",
            "        Median Score: -9.106289056148853e-10\n",
            "        Max Score: 6.263820750973537e-07  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "\n",
            "Grasp10 Pruning | fc.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc.weight, Layer Size: 1000\n",
            "        Min Score: -8.312683348776773e-06\n",
            "        Average Score: -4.791148171534587e-07\n",
            "        Median Score: -1.1531928834074279e-07\n",
            "        Max Score: 7.766379894746933e-07  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "Training\n",
            "test\tep 000 it 000 loss 2.301 acc1 10.98% acc5 51.12% ex 10000 time 0.00s\n",
            "test\tep 001 it 000 loss 0.434 acc1 87.03% acc5 98.89% ex 10000 time 18.07s\n",
            "test\tep 002 it 000 loss 0.332 acc1 90.06% acc5 99.34% ex 10000 time 17.98s\n",
            "test\tep 003 it 000 loss 0.284 acc1 91.44% acc5 99.46% ex 10000 time 17.97s\n",
            "test\tep 004 it 000 loss 0.250 acc1 92.76% acc5 99.54% ex 10000 time 18.16s\n",
            "test\tep 005 it 000 loss 0.232 acc1 93.25% acc5 99.60% ex 10000 time 18.01s\n",
            "test\tep 006 it 000 loss 0.218 acc1 93.55% acc5 99.61% ex 10000 time 18.01s\n",
            "test\tep 007 it 000 loss 0.209 acc1 93.85% acc5 99.66% ex 10000 time 18.06s\n",
            "test\tep 008 it 000 loss 0.195 acc1 94.23% acc5 99.72% ex 10000 time 18.14s\n",
            "test\tep 009 it 000 loss 0.189 acc1 94.38% acc5 99.77% ex 10000 time 18.13s\n",
            "test\tep 010 it 000 loss 0.179 acc1 94.80% acc5 99.80% ex 10000 time 18.01s\n",
            "test\tep 011 it 000 loss 0.176 acc1 94.82% acc5 99.75% ex 10000 time 18.06s\n",
            "test\tep 012 it 000 loss 0.180 acc1 94.89% acc5 99.72% ex 10000 time 18.12s\n",
            "test\tep 013 it 000 loss 0.169 acc1 94.83% acc5 99.79% ex 10000 time 18.10s\n",
            "test\tep 014 it 000 loss 0.168 acc1 94.97% acc5 99.76% ex 10000 time 18.24s\n",
            "test\tep 015 it 000 loss 0.166 acc1 95.02% acc5 99.75% ex 10000 time 18.25s\n",
            "test\tep 016 it 000 loss 0.157 acc1 95.21% acc5 99.77% ex 10000 time 18.18s\n",
            "test\tep 017 it 000 loss 0.156 acc1 95.27% acc5 99.82% ex 10000 time 18.17s\n",
            "test\tep 018 it 000 loss 0.159 acc1 95.33% acc5 99.84% ex 10000 time 17.98s\n",
            "test\tep 019 it 000 loss 0.152 acc1 95.49% acc5 99.85% ex 10000 time 18.20s\n",
            "test\tep 020 it 000 loss 0.157 acc1 95.19% acc5 99.78% ex 10000 time 18.10s\n",
            "test\tep 021 it 000 loss 0.153 acc1 95.47% acc5 99.82% ex 10000 time 18.21s\n",
            "test\tep 022 it 000 loss 0.147 acc1 95.69% acc5 99.83% ex 10000 time 18.06s\n",
            "test\tep 023 it 000 loss 0.151 acc1 95.50% acc5 99.81% ex 10000 time 17.95s\n",
            "test\tep 024 it 000 loss 0.151 acc1 95.52% acc5 99.81% ex 10000 time 18.15s\n",
            "test\tep 025 it 000 loss 0.146 acc1 95.72% acc5 99.81% ex 10000 time 18.05s\n",
            "test\tep 026 it 000 loss 0.143 acc1 95.80% acc5 99.81% ex 10000 time 18.00s\n",
            "test\tep 027 it 000 loss 0.141 acc1 95.79% acc5 99.81% ex 10000 time 18.28s\n",
            "test\tep 028 it 000 loss 0.147 acc1 95.74% acc5 99.82% ex 10000 time 18.26s\n",
            "test\tep 029 it 000 loss 0.156 acc1 95.55% acc5 99.85% ex 10000 time 18.10s\n",
            "test\tep 030 it 000 loss 0.141 acc1 95.86% acc5 99.83% ex 10000 time 18.10s\n",
            "test\tep 031 it 000 loss 0.135 acc1 96.03% acc5 99.84% ex 10000 time 18.04s\n",
            "test\tep 032 it 000 loss 0.139 acc1 95.81% acc5 99.83% ex 10000 time 18.27s\n",
            "test\tep 033 it 000 loss 0.157 acc1 95.30% acc5 99.80% ex 10000 time 18.10s\n",
            "test\tep 034 it 000 loss 0.142 acc1 95.99% acc5 99.84% ex 10000 time 18.09s\n",
            "test\tep 035 it 000 loss 0.138 acc1 95.92% acc5 99.84% ex 10000 time 18.15s\n",
            "test\tep 036 it 000 loss 0.137 acc1 95.96% acc5 99.86% ex 10000 time 18.19s\n",
            "test\tep 037 it 000 loss 0.137 acc1 96.02% acc5 99.86% ex 10000 time 18.23s\n",
            "test\tep 038 it 000 loss 0.141 acc1 95.78% acc5 99.88% ex 10000 time 18.31s\n",
            "test\tep 039 it 000 loss 0.135 acc1 96.10% acc5 99.88% ex 10000 time 18.14s\n",
            "test\tep 040 it 000 loss 0.140 acc1 96.07% acc5 99.87% ex 10000 time 18.00s\n",
            "==================================================================================\n",
            "Branch oneshot_experiments (Replicate 1)\n",
            "----------------------------------------------------------------------------------\n",
            "Dataset Hyperparameters\n",
            "    * dataset_name => mnist\n",
            "    * batch_size => 128\n",
            "Model Hyperparameters\n",
            "    * model_name => mnist_lenet_300_100\n",
            "    * model_init => kaiming_normal\n",
            "    * batchnorm_init => uniform\n",
            "Training Hyperparameters\n",
            "    * optimizer_name => sgd\n",
            "    * lr => 0.1\n",
            "    * training_steps => 40ep\n",
            "Branch Arguments\n",
            "    * strategy => graspabs10\n",
            "    * prune_fraction => 0.75\n",
            "    * prune_experiment => main\n",
            "    * prune_step => 0ep0it\n",
            "    * prune_iterations => 1\n",
            "    * state_experiment => main\n",
            "    * state_step => 0ep0it\n",
            "    * start_step => 0ep0it\n",
            "Output Location: /root/open_lth_data2/train_574e51abc295d8da78175b320504f2ba/replicate_1/train_branch_oneshot_experiments_6c5d3865b2b31ceeec78996f69f07772\n",
            "==================================================================================\n",
            "\n",
            "prune iteration 1\n",
            "grasp 0 \n",
            "\n",
            "Graspabs10 Pruning | fc_layers.0.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.0.weight, Layer Size: 235200\n",
            "        Min Score: 2.3347798270201194e-15\n",
            "        Average Score: 5.9097611249114834e-09\n",
            "        Median Score: 2.5712565410174193e-09\n",
            "        Max Score: 2.6075935011249385e-07  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "\n",
            "Graspabs10 Pruning | fc_layers.1.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.1.weight, Layer Size: 30000\n",
            "        Min Score: 0.0\n",
            "        Average Score: 2.989995451230243e-08\n",
            "        Median Score: 9.898730723989502e-09\n",
            "        Max Score: 1.3496537576429546e-06  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "\n",
            "Graspabs10 Pruning | fc.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc.weight, Layer Size: 1000\n",
            "        Min Score: 6.173884320448408e-12\n",
            "        Average Score: 5.290135050017852e-07\n",
            "        Median Score: 1.726229470477847e-07\n",
            "        Max Score: 8.312683348776773e-06  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "Training\n",
            "test\tep 000 it 000 loss 4.900 acc1 8.93% acc5 54.26% ex 10000 time 0.00s\n",
            "test\tep 001 it 000 loss 0.188 acc1 94.28% acc5 99.75% ex 10000 time 18.10s\n",
            "test\tep 002 it 000 loss 0.132 acc1 96.00% acc5 99.87% ex 10000 time 18.06s\n",
            "test\tep 003 it 000 loss 0.160 acc1 94.96% acc5 99.90% ex 10000 time 18.12s\n",
            "test\tep 004 it 000 loss 0.102 acc1 96.87% acc5 99.91% ex 10000 time 18.20s\n",
            "test\tep 005 it 000 loss 0.097 acc1 96.89% acc5 99.94% ex 10000 time 18.19s\n",
            "test\tep 006 it 000 loss 0.090 acc1 97.24% acc5 99.95% ex 10000 time 18.03s\n",
            "test\tep 007 it 000 loss 0.090 acc1 97.26% acc5 99.93% ex 10000 time 18.15s\n",
            "test\tep 008 it 000 loss 0.087 acc1 97.37% acc5 99.94% ex 10000 time 18.43s\n",
            "test\tep 009 it 000 loss 0.084 acc1 97.47% acc5 99.94% ex 10000 time 18.24s\n",
            "test\tep 010 it 000 loss 0.095 acc1 97.05% acc5 99.94% ex 10000 time 18.65s\n",
            "test\tep 011 it 000 loss 0.083 acc1 97.59% acc5 99.92% ex 10000 time 18.15s\n",
            "test\tep 012 it 000 loss 0.087 acc1 97.52% acc5 99.95% ex 10000 time 18.10s\n",
            "test\tep 013 it 000 loss 0.081 acc1 97.69% acc5 99.94% ex 10000 time 18.12s\n",
            "test\tep 014 it 000 loss 0.083 acc1 97.57% acc5 99.95% ex 10000 time 18.18s\n",
            "test\tep 015 it 000 loss 0.090 acc1 97.48% acc5 99.94% ex 10000 time 18.12s\n",
            "test\tep 016 it 000 loss 0.087 acc1 97.58% acc5 99.95% ex 10000 time 18.17s\n",
            "test\tep 017 it 000 loss 0.088 acc1 97.57% acc5 99.95% ex 10000 time 18.04s\n",
            "test\tep 018 it 000 loss 0.088 acc1 97.69% acc5 99.93% ex 10000 time 18.04s\n",
            "test\tep 019 it 000 loss 0.087 acc1 97.71% acc5 99.95% ex 10000 time 18.11s\n",
            "test\tep 020 it 000 loss 0.088 acc1 97.61% acc5 99.95% ex 10000 time 18.04s\n",
            "test\tep 021 it 000 loss 0.089 acc1 97.65% acc5 99.93% ex 10000 time 17.99s\n",
            "test\tep 022 it 000 loss 0.090 acc1 97.73% acc5 99.95% ex 10000 time 18.09s\n",
            "test\tep 023 it 000 loss 0.091 acc1 97.62% acc5 99.93% ex 10000 time 18.13s\n",
            "test\tep 024 it 000 loss 0.093 acc1 97.71% acc5 99.94% ex 10000 time 17.91s\n",
            "test\tep 025 it 000 loss 0.094 acc1 97.68% acc5 99.94% ex 10000 time 18.09s\n",
            "test\tep 026 it 000 loss 0.096 acc1 97.71% acc5 99.95% ex 10000 time 18.11s\n",
            "test\tep 027 it 000 loss 0.094 acc1 97.74% acc5 99.95% ex 10000 time 18.19s\n",
            "test\tep 028 it 000 loss 0.095 acc1 97.66% acc5 99.94% ex 10000 time 18.06s\n",
            "test\tep 029 it 000 loss 0.095 acc1 97.69% acc5 99.94% ex 10000 time 18.03s\n",
            "test\tep 030 it 000 loss 0.096 acc1 97.73% acc5 99.94% ex 10000 time 18.14s\n",
            "test\tep 031 it 000 loss 0.097 acc1 97.69% acc5 99.95% ex 10000 time 18.15s\n",
            "test\tep 032 it 000 loss 0.099 acc1 97.71% acc5 99.94% ex 10000 time 18.05s\n",
            "test\tep 033 it 000 loss 0.099 acc1 97.67% acc5 99.94% ex 10000 time 18.07s\n",
            "test\tep 034 it 000 loss 0.099 acc1 97.72% acc5 99.94% ex 10000 time 18.08s\n",
            "test\tep 035 it 000 loss 0.100 acc1 97.67% acc5 99.94% ex 10000 time 18.03s\n",
            "test\tep 036 it 000 loss 0.100 acc1 97.80% acc5 99.94% ex 10000 time 18.07s\n",
            "test\tep 037 it 000 loss 0.101 acc1 97.74% acc5 99.95% ex 10000 time 18.08s\n",
            "test\tep 038 it 000 loss 0.101 acc1 97.74% acc5 99.94% ex 10000 time 18.35s\n",
            "test\tep 039 it 000 loss 0.103 acc1 97.68% acc5 99.94% ex 10000 time 18.17s\n",
            "test\tep 040 it 000 loss 0.103 acc1 97.70% acc5 99.94% ex 10000 time 18.07s\n",
            "==================================================================================\n",
            "Branch oneshot_experiments (Replicate 1)\n",
            "----------------------------------------------------------------------------------\n",
            "Dataset Hyperparameters\n",
            "    * dataset_name => mnist\n",
            "    * batch_size => 128\n",
            "Model Hyperparameters\n",
            "    * model_name => mnist_lenet_300_100\n",
            "    * model_init => kaiming_normal\n",
            "    * batchnorm_init => uniform\n",
            "Training Hyperparameters\n",
            "    * optimizer_name => sgd\n",
            "    * lr => 0.1\n",
            "    * training_steps => 40ep\n",
            "Branch Arguments\n",
            "    * strategy => synflow\n",
            "    * prune_fraction => 0.75\n",
            "    * prune_experiment => main\n",
            "    * prune_step => 0ep0it\n",
            "    * prune_iterations => 1\n",
            "    * state_experiment => main\n",
            "    * state_step => 0ep0it\n",
            "    * start_step => 0ep0it\n",
            "Output Location: /root/open_lth_data2/train_574e51abc295d8da78175b320504f2ba/replicate_1/train_branch_oneshot_experiments_e89d9ef168eba1e136c66094175176f7\n",
            "==================================================================================\n",
            "\n",
            "prune iteration 1\n",
            "\n",
            "Synflow Pruning | fc_layers.0.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.0.weight, Layer Size: 235200\n",
            "        Min Score: 5.506025165404749e-07\n",
            "        Average Score: 0.2994634313974715\n",
            "        Median Score: 0.2519735762452648\n",
            "        Max Score: 1.9064072617471266  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\n",
            "Synflow Pruning | fc_layers.1.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc_layers.1.weight, Layer Size: 30000\n",
            "        Min Score: 0.0003836303781037737\n",
            "        Average Score: 2.349089649698988\n",
            "        Median Score: 1.9202726808278179\n",
            "        Max Score: 13.975855851447085  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\n",
            "Synflow Pruning | fc.weight\n",
            "Prune %: 0.75 | Prune Iterations: 1\n",
            "        Layer Name: fc.weight, Layer Size: 1000\n",
            "        Min Score: 0.12718651687917532\n",
            "        Average Score: 70.47561969543244\n",
            "        Median Score: 63.40477697655131\n",
            "        Max Score: 267.29954409428217  \n",
            "        \n",
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Training\n",
            "test\tep 000 it 000 loss 3.623 acc1 8.99% acc5 52.22% ex 10000 time 0.00s\n",
            "test\tep 001 it 000 loss 0.197 acc1 94.03% acc5 99.70% ex 10000 time 17.43s\n",
            "test\tep 002 it 000 loss 0.150 acc1 95.33% acc5 99.88% ex 10000 time 17.60s\n",
            "test\tep 003 it 000 loss 0.128 acc1 95.97% acc5 99.91% ex 10000 time 17.41s\n",
            "test\tep 004 it 000 loss 0.110 acc1 96.66% acc5 99.90% ex 10000 time 17.64s\n",
            "test\tep 005 it 000 loss 0.116 acc1 96.28% acc5 99.90% ex 10000 time 17.44s\n",
            "test\tep 006 it 000 loss 0.092 acc1 97.10% acc5 99.91% ex 10000 time 17.54s\n",
            "test\tep 007 it 000 loss 0.089 acc1 97.22% acc5 99.91% ex 10000 time 17.63s\n",
            "test\tep 008 it 000 loss 0.093 acc1 97.15% acc5 99.93% ex 10000 time 17.47s\n",
            "test\tep 009 it 000 loss 0.087 acc1 97.36% acc5 99.94% ex 10000 time 17.54s\n",
            "test\tep 010 it 000 loss 0.081 acc1 97.46% acc5 99.93% ex 10000 time 17.44s\n",
            "test\tep 011 it 000 loss 0.087 acc1 97.35% acc5 99.93% ex 10000 time 17.65s\n",
            "test\tep 012 it 000 loss 0.080 acc1 97.48% acc5 99.93% ex 10000 time 17.62s\n",
            "test\tep 013 it 000 loss 0.079 acc1 97.58% acc5 99.92% ex 10000 time 17.58s\n",
            "test\tep 014 it 000 loss 0.083 acc1 97.47% acc5 99.93% ex 10000 time 17.66s\n",
            "test\tep 015 it 000 loss 0.078 acc1 97.67% acc5 99.91% ex 10000 time 17.64s\n",
            "test\tep 016 it 000 loss 0.079 acc1 97.62% acc5 99.93% ex 10000 time 17.69s\n",
            "test\tep 017 it 000 loss 0.081 acc1 97.68% acc5 99.92% ex 10000 time 17.63s\n",
            "test\tep 018 it 000 loss 0.084 acc1 97.51% acc5 99.91% ex 10000 time 17.56s\n",
            "test\tep 019 it 000 loss 0.084 acc1 97.60% acc5 99.92% ex 10000 time 17.62s\n",
            "test\tep 020 it 000 loss 0.082 acc1 97.68% acc5 99.91% ex 10000 time 17.67s\n",
            "test\tep 021 it 000 loss 0.082 acc1 97.71% acc5 99.92% ex 10000 time 17.37s\n",
            "test\tep 022 it 000 loss 0.084 acc1 97.68% acc5 99.93% ex 10000 time 17.66s\n",
            "test\tep 023 it 000 loss 0.082 acc1 97.68% acc5 99.93% ex 10000 time 17.56s\n",
            "test\tep 024 it 000 loss 0.084 acc1 97.69% acc5 99.93% ex 10000 time 17.57s\n",
            "test\tep 025 it 000 loss 0.085 acc1 97.69% acc5 99.92% ex 10000 time 17.62s\n",
            "test\tep 026 it 000 loss 0.085 acc1 97.68% acc5 99.92% ex 10000 time 17.41s\n",
            "test\tep 027 it 000 loss 0.085 acc1 97.69% acc5 99.94% ex 10000 time 17.61s\n",
            "test\tep 028 it 000 loss 0.087 acc1 97.78% acc5 99.91% ex 10000 time 17.64s\n",
            "test\tep 029 it 000 loss 0.088 acc1 97.70% acc5 99.91% ex 10000 time 17.51s\n",
            "test\tep 030 it 000 loss 0.088 acc1 97.68% acc5 99.93% ex 10000 time 17.49s\n",
            "test\tep 031 it 000 loss 0.088 acc1 97.73% acc5 99.92% ex 10000 time 17.54s\n",
            "test\tep 032 it 000 loss 0.091 acc1 97.69% acc5 99.93% ex 10000 time 17.52s\n",
            "test\tep 033 it 000 loss 0.090 acc1 97.70% acc5 99.92% ex 10000 time 17.62s\n",
            "test\tep 034 it 000 loss 0.091 acc1 97.68% acc5 99.94% ex 10000 time 17.61s\n",
            "test\tep 035 it 000 loss 0.090 acc1 97.69% acc5 99.94% ex 10000 time 17.50s\n",
            "test\tep 036 it 000 loss 0.091 acc1 97.76% acc5 99.92% ex 10000 time 17.49s\n",
            "test\tep 037 it 000 loss 0.092 acc1 97.79% acc5 99.92% ex 10000 time 17.63s\n",
            "test\tep 038 it 000 loss 0.093 acc1 97.72% acc5 99.92% ex 10000 time 17.93s\n",
            "test\tep 039 it 000 loss 0.094 acc1 97.76% acc5 99.92% ex 10000 time 17.53s\n",
            "test\tep 040 it 000 loss 0.093 acc1 97.79% acc5 99.92% ex 10000 time 17.94s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8hMP2CD3JbN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}